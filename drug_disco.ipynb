{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit-pypi\n",
      "  Downloading rdkit_pypi-2022.9.5-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numpy in e:\\iitb\\sem8\\me793\\drug discovery\\drug-discovery\\.venv\\lib\\site-packages (from rdkit-pypi) (1.26.4)\n",
      "Collecting Pillow (from rdkit-pypi)\n",
      "  Downloading pillow-10.3.0-cp311-cp311-win_amd64.whl.metadata (9.4 kB)\n",
      "Downloading rdkit_pypi-2022.9.5-cp311-cp311-win_amd64.whl (20.5 MB)\n",
      "   ---------------------------------------- 0.0/20.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/20.5 MB 3.2 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 0.2/20.5 MB 2.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.8/20.5 MB 6.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.2/20.5 MB 7.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.7/20.5 MB 8.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.3/20.5 MB 8.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.8/20.5 MB 8.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.1/20.5 MB 9.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.7/20.5 MB 9.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 4.3/20.5 MB 9.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 4.9/20.5 MB 9.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 5.4/20.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 5.8/20.5 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 6.0/20.5 MB 9.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 6.3/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 6.7/20.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 6.9/20.5 MB 8.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 7.4/20.5 MB 8.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 7.9/20.5 MB 9.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 8.4/20.5 MB 9.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.9/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.4/20.5 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 9.9/20.5 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 10.5/20.5 MB 6.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 11.1/20.5 MB 6.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 11.7/20.5 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 11.8/20.5 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 12.4/20.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 13.0/20.5 MB 6.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 13.6/20.5 MB 6.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 14.2/20.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 14.7/20.5 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 15.2/20.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 15.7/20.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 16.3/20.5 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.9/20.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 17.4/20.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 17.8/20.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 18.4/20.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 18.8/20.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 19.2/20.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.7/20.5 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.2/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  20.4/20.5 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 20.5/20.5 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading pillow-10.3.0-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB 9.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.6/2.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.5 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.1/2.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.0/2.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: Pillow, rdkit-pypi\n",
      "Successfully installed Pillow-10.3.0 rdkit-pypi-2022.9.5\n"
     ]
    }
   ],
   "source": [
    "# !pip -q install rdkit-pypi==2023.9.5\n",
    "!pip install rdkit-pypi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem.Draw import MolsToGridImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>logP</th>\n",
       "      <th>qed</th>\n",
       "      <th>SAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1</td>\n",
       "      <td>5.05060</td>\n",
       "      <td>0.702012</td>\n",
       "      <td>2.084095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1</td>\n",
       "      <td>3.11370</td>\n",
       "      <td>0.928975</td>\n",
       "      <td>3.432004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...</td>\n",
       "      <td>4.96778</td>\n",
       "      <td>0.599682</td>\n",
       "      <td>2.470633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...</td>\n",
       "      <td>4.00022</td>\n",
       "      <td>0.690944</td>\n",
       "      <td>2.822753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...</td>\n",
       "      <td>3.60956</td>\n",
       "      <td>0.789027</td>\n",
       "      <td>4.035182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles     logP       qed  \\\n",
       "0            CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1  5.05060  0.702012   \n",
       "1       C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1  3.11370  0.928975   \n",
       "2  N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...  4.96778  0.599682   \n",
       "3  CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...  4.00022  0.690944   \n",
       "4  N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...  3.60956  0.789027   \n",
       "\n",
       "        SAS  \n",
       "0  2.084095  \n",
       "1  3.432004  \n",
       "2  2.470633  \n",
       "3  2.822753  \n",
       "4  4.035182  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"dataset\\250k_rndm_zinc_drugs_clean_3.csv\")\n",
    "df['smiles'] = df['smiles'].apply(lambda s: s.replace('\\n', ''))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecules_from_smile(smiles):\n",
    "    molecule = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "\n",
    "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "        Chem.SanitizeMol(molecule, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL ^ flag)\n",
    "\n",
    "    Chem.AssignStereochemistry(molecule, cleanIt=True, force=True)\n",
    "\n",
    "    return molecule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES:\tCOc1ccc([C@H]2C[C@@H](C(F)(F)F)n3nc(C(=O)NC4CCCCC4)cc3N2)cc1OC\n",
      "logP:\t4.6231\n",
      "qed:\t0.661591299118\n",
      "Molecule:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x1ea69e6c820>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"SMILES:\\t{df.smiles[100]}\\nlogP:\\t{df.logP[100]}\\nqed:\\t{df.qed[100]}\")\n",
    "molecule = molecules_from_smile(df.smiles[100])\n",
    "print(\"Molecule:\")\n",
    "molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max molecule size: 109\n",
      "Character set Length: 11\n"
     ]
    }
   ],
   "source": [
    "SMILE_CHARSET = '[\"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \\\n",
    "                  \"P\", \"Cl\", \"Br\"]'\n",
    "\n",
    "bond_mapping = {\n",
    "    'SINGLE': 0,\n",
    "    0: Chem.BondType.SINGLE,\n",
    "    \"DOUBLE\": 1,\n",
    "    1: Chem.BondType.DOUBLE,\n",
    "    'TRIPLE': 2,\n",
    "    0: Chem.BondType.TRIPLE,\n",
    "    \"AROMATIC\": 3,\n",
    "    1: Chem.BondType.AROMATIC,\n",
    "}\n",
    "\n",
    "SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n",
    "# type(SMILE_CHARSET)\n",
    "\n",
    "MAX_MOLSIZE = max(df['smiles'].str.len())\n",
    "SMILE_to_index = dict((c,i) for i, c in enumerate(SMILE_CHARSET))\n",
    "index_to_SMILE = dict((i,c) for i, c in enumerate(SMILE_CHARSET))\n",
    "atom_mapping = dict(SMILE_to_index)\n",
    "atom_mapping.update(index_to_SMILE)\n",
    "\n",
    "# print(SMILE_to_index)\n",
    "# print(index_to_SMILE)\n",
    "# print(atom_mapping)\n",
    "\n",
    "print(\"Max molecule size: {}\".format(MAX_MOLSIZE))\n",
    "print(\"Character set Length: {}\".format(len(SMILE_CHARSET)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS  = 10\n",
    "\n",
    "VAE_LR = 5e-4\n",
    "NUM_ATOMS = 120\n",
    "\n",
    "ATOM_DIM = len(SMILE_CHARSET)\n",
    "BOND_DIM = 4 + 1\n",
    "LATENT_DIM = 435\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_graph(smiles):\n",
    "    # Converts SMILES to molecule object\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Initialize adjacency and feature tensor\n",
    "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
    "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
    "\n",
    "    # loop over each atom in molecule\n",
    "    for atom in molecule.GetAtoms():\n",
    "        i = atom.GetIdx()\n",
    "        atom_type = atom_mapping[atom.GetSymbol()]\n",
    "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
    "        # loop over one-hop neighbors\n",
    "        for neighbor in atom.GetNeighbors():\n",
    "            j = neighbor.GetIdx()\n",
    "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
    "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
    "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
    "\n",
    "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
    "    # Notice: channels-first\n",
    "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
    "\n",
    "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
    "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
    "\n",
    "    return adjacency, features\n",
    "\n",
    "def graph_to_molecule(graph):\n",
    "    # Unpack graph\n",
    "    adjacency, features = graph\n",
    "\n",
    "    # RWMol is a molecule object intended to be edited\n",
    "    molecule = Chem.RWMol()\n",
    "\n",
    "    # Remove \"no atoms\" & atoms with no bonds\n",
    "    keep_idx = np.where(\n",
    "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
    "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
    "    )[0]\n",
    "    features = features[keep_idx]\n",
    "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
    "\n",
    "    # Add atoms to molecule\n",
    "    for atom_type_idx in np.argmax(features, axis=1):\n",
    "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
    "        _ = molecule.AddAtom(atom)\n",
    "\n",
    "    # Add bonds between atoms in molecule; based on the upper triangles\n",
    "    # of the [symmetric] adjacency tensor\n",
    "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
    "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
    "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
    "            continue\n",
    "        bond_type = bond_mapping[bond_ij]\n",
    "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
    "\n",
    "    # Sanitize the molecule; for more information on sanitization, see\n",
    "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
    "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "    # Let's be strict. If sanitization fails, return None\n",
    "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "        return None\n",
    "\n",
    "    return molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self,data, mapping, max_len, batch_size = 6, shuffle = True):\n",
    "        self.data = data\n",
    "        self.indices = self.data.index.tolist()\n",
    "        self.mapping = mapping\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if (index + 1) * self.batch_size > len(self.indices):\n",
    "            self.batch_size = len(self.indices) - index * self.batch_size\n",
    "\n",
    "        index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        mol_features, mol_property = self.data_generation(batch)\n",
    "\n",
    "        return mol_features, mol_property\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "        \n",
    "    def load(self, idx):\n",
    "        \"\"\"\n",
    "        Load molecules adjacency matrix and features matrix from SMILE representation \n",
    "        and their respective SAS value.\n",
    "        \"\"\" \n",
    "        qed = self.data.loc[idx]['qed']\n",
    "        adjacency, features = smiles_to_graph(self.data.loc[idx]['smiles'])\n",
    "\n",
    "        return adjacency, features, qed\n",
    "    \n",
    "    def data_generation(self, batch):\n",
    "\n",
    "        x1 = np.empty((self.batch_size, BOND_DIM, self.max_len, self.max_len))\n",
    "        x2 = np.empty((self.batch_size, self.max_len, len(self.mapping)))\n",
    "        x3 = np.empty((self.batch_size,))\n",
    "\n",
    "        for i, batch_id in enumerate(batch):\n",
    "            x1[i,], x2[i,], x3[i,] = self.load(batch_id)\n",
    "            return [x1,x2], x3\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.75, random_state=42)\n",
    "\n",
    "test_df = df.drop(train_df.index)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RelationalGraphConvLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            units = 128,\n",
    "            activation = 'relu',\n",
    "            use_bias = False,\n",
    "            kernel_initializer = 'glorot_uniform',\n",
    "            bias_initializer = 'zeros',\n",
    "            kernel_regularizer = None,\n",
    "            bias_regularizer = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = getattr(F, activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        if self.kernel_initialier == 'glorot_uniform':\n",
    "            nn.init.xavier_uniform_(self.weight)\n",
    "        elif self.kernel_initializer == 'glorot_normal':\n",
    "            nn.init.xavier_normal_(self.weight)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Kernel initializer\")\n",
    "        \n",
    "        if self.use_bias:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        bond_dim = input_shape[0][1]\n",
    "        atom_dim = input_shape[1][2]\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(bond_dim, atom_dim, self.units))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(bond_dim, 1, self.units))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        adjacency, features = inputs\n",
    "\n",
    "        X = torch.matmul(adjacency, features.unsqueeze(1))\n",
    "\n",
    "        X = torch.matmul(x, self.weight)\n",
    "        if self.use_bias:\n",
    "            x += self.bias\n",
    "        x_reduced = torch.sum(x, dim=1)\n",
    "        return self.activation(x_reduced)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate):\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate):\n",
    "            super(Encoder, self).__init__()\n",
    "\n",
    "            # Define graph convolutional layers\n",
    "            self.gconv_layers = nn.ModuleList([RelationalGraphConvLayer(units) for units in gconv_units])\n",
    "\n",
    "            # Define dense layers\n",
    "            dense_layers = []\n",
    "            input_size = gconv_units[-1]  # Input size for the first dense layer\n",
    "            for units in dense_units:\n",
    "                dense_layers.append(nn.Linear(input_size, units))\n",
    "                dense_layers.append(nn.ReLU())\n",
    "                dense_layers.append(nn.Dropout(dropout_rate))\n",
    "                input_size = units\n",
    "            self.dense_layers = nn.Sequential(*dense_layers)\n",
    "\n",
    "            # Define linear layers for z_mean and log_var\n",
    "            self.z_mean_layer = nn.Linear(input_size, latent_dim)\n",
    "            self.log_var_layer = nn.Linear(input_size, latent_dim)\n",
    "\n",
    "        def forward(self, adjacency, features):\n",
    "            # Propagate through graph convolutional layers\n",
    "            features_transformed = features\n",
    "            for gconv_layer in self.gconv_layers:\n",
    "                features_transformed = gconv_layer([adjacency, features_transformed])\n",
    "\n",
    "            # Reduce 2-D representation of molecule to 1-D\n",
    "            x = torch.mean(features_transformed, dim=1)\n",
    "\n",
    "            # Propagate through densely connected layers\n",
    "            x = self.dense_layers(x)\n",
    "\n",
    "            # Compute z_mean and log_var\n",
    "            z_mean = self.z_mean_layer(x)\n",
    "            log_var = self.log_var_layer(x)\n",
    "\n",
    "            return z_mean, log_var\n",
    "\n",
    "    # Instantiate the encoder\n",
    "    encoder = Encoder(gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate)\n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n",
    "            super(Decoder, self).__init__()\n",
    "\n",
    "            # Define dense layers\n",
    "            dense_layers = []\n",
    "            input_size = latent_dim\n",
    "            for units in dense_units:\n",
    "                dense_layers.append(nn.Linear(input_size, units))\n",
    "                dense_layers.append(nn.Tanh())\n",
    "                dense_layers.append(nn.Dropout(dropout_rate))\n",
    "                input_size = units\n",
    "            self.dense_layers = nn.Sequential(*dense_layers)\n",
    "\n",
    "            # Define linear layers for adjacency and feature tensors\n",
    "            self.adjacency_layer = nn.Linear(input_size, torch.prod(torch.tensor(adjacency_shape)))\n",
    "            self.feature_layer = nn.Linear(input_size, torch.prod(torch.tensor(feature_shape)))\n",
    "\n",
    "        def forward(self, latent_inputs):\n",
    "            # Propagate through densely connected layers\n",
    "            x = self.dense_layers(latent_inputs)\n",
    "\n",
    "            # Map outputs of previous layer (x) to continuous adjacency tensor\n",
    "            x_adjacency = self.adjacency_layer(x)\n",
    "            x_adjacency = x_adjacency.view(-1, *adjacency_shape)\n",
    "            # Symmetrify tensors in the last two dimensions\n",
    "            x_adjacency = (x_adjacency + x_adjacency.transpose(1, 2)) / 2\n",
    "            x_adjacency = F.softmax(x_adjacency, dim=1)\n",
    "\n",
    "            # Map outputs of previous layer (x) to continuous feature tensor\n",
    "            x_features = self.feature_layer(x)\n",
    "            x_features = x_features.view(-1, *feature_shape)\n",
    "            x_features = F.softmax(x_features, dim=2)\n",
    "\n",
    "            return x_adjacency, x_features\n",
    "\n",
    "    # Instantiate the decoder\n",
    "    decoder = Decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape)\n",
    "    return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding.\"\"\"\n",
    "\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        batch = z_log_var.size(0)\n",
    "        dim = z_log_var.size(1)\n",
    "        epsilon = torch.randn(batch, dim, device=z_log_var.device)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoleculeGenerator(nn.Module):\n",
    "    def __init__(self, encoder, decoder, max_len):\n",
    "        super(MoleculeGenerator, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        # self.property_prediction_layer = nn.Linear(encoder.latent_dim, 1)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, mol_features):\n",
    "        z_mean, z_log_var = self.encoder(mol_features)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "\n",
    "        reconstruction_adjacency, reconstruction_features = self.decoder(z)\n",
    "\n",
    "        property_prediction = nn.Linear(z_mean, 1)\n",
    "\n",
    "        return z_mean, z_log_var, property_prediction, reconstruction_adjacency, reconstruction_features\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def calculate_loss(self, z_mean, z_log_var, mol_property, property_prediction, graph_real, graph_generated, is_train):\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_generated, features_generated = graph_generated\n",
    "\n",
    "        adjacency_reconstruction_loss = F.binary_cross_entropy(adjacency_generated, adjacency_real)\n",
    "        features_reconstruction_loss = F.cross_entropy(features_generated, features_real.argmax(dim=2))\n",
    "\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), 1).mean()\n",
    "\n",
    "        property_prediction_loss = F.binary_cross_entropy_with_logits(property_prediction, mol_property)\n",
    "\n",
    "        if is_train:\n",
    "            graph_loss = self.gradient_penalty(graph_real, graph_generated)\n",
    "        else:\n",
    "            graph_loss = 0\n",
    "\n",
    "        return kl_loss + property_prediction_loss + graph_loss + adjacency_reconstruction_loss + features_reconstruction_loss\n",
    "\n",
    "    def gradient_penalty(self, graph_real, graph_generated):\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_generated, features_generated = graph_generated\n",
    "\n",
    "        alpha = torch.rand(adjacency_real.size(0), 1, 1, 1)\n",
    "        alpha = alpha.to(adjacency_real.device)\n",
    "\n",
    "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
    "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
    "\n",
    "        adjacency_interp.requires_grad = True\n",
    "        features_interp.requires_grad = True\n",
    "\n",
    "        _, _, logits, _, _ = self([adjacency_interp, features_interp])\n",
    "\n",
    "        grads = torch.autograd.grad(outputs=logits, inputs=[adjacency_interp, features_interp], grad_outputs=torch.ones_like(logits), create_graph=True)\n",
    "\n",
    "        grads_adjacency_penalty = (1 - grads[0].norm(dim=1)) ** 2\n",
    "        grads_features_penalty = (1 - grads[1].norm(dim=2)) ** 2\n",
    "\n",
    "        return torch.mean(torch.mean(grads_adjacency_penalty, dim=(-2, -1)) + torch.mean(grads_features_penalty, dim=(-1)))\n",
    "\n",
    "    def inference(self, batch_size):\n",
    "        z = torch.randn(batch_size, self.encoder.latent_dim)\n",
    "        reconstruction_adjacency, reconstruction_features = self.decoder(z)\n",
    "\n",
    "        adjacency = reconstruction_adjacency.argmax(dim=1)\n",
    "        adjacency = F.one_hot(adjacency, num_classes=BOND_DIM)\n",
    "\n",
    "        adjacency = adjacency - torch.diag(torch.diagonal(adjacency))\n",
    "\n",
    "        features = reconstruction_features.argmax(dim=2)\n",
    "        features = F.one_hot(features, num_classes=ATOM_DIM)\n",
    "\n",
    "        return [\n",
    "            graph_to_molecule([adjacency[i].detach().numpy(), features[i].detach().numpy()])\n",
    "            for i in range(batch_size)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataGenerator(\n",
    "    data=train_df[:8000],\n",
    "    mapping = SMILE_to_index, \n",
    "    max_len = NUM_ATOMS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_loader = DataGenerator(\n",
    "    data=test_df[:8000],\n",
    "    mapping = SMILE_to_index, \n",
    "    max_len = NUM_ATOMS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([[[[0., 1., 0., ..., 0., 0., 0.],\n",
      "         [1., 0., 1., ..., 0., 0., 0.],\n",
      "         [0., 1., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 1., ..., 1., 1., 1.],\n",
      "         [0., 1., 0., ..., 1., 1., 1.],\n",
      "         [1., 0., 1., ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1., ..., 1., 1., 1.],\n",
      "         [1., 1., 1., ..., 1., 1., 1.],\n",
      "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]]), array([[[1.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "        [1.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "        ...,\n",
      "        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 1.00000000e+000],\n",
      "        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 1.00000000e+000],\n",
      "        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 1.00000000e+000]],\n",
      "\n",
      "       [[            nan, 1.88559060e-263,             nan, ...,\n",
      "         7.22398477e-287,             nan,             nan],\n",
      "        [1.90543892e-261, 2.08471066e-283,             nan, ...,\n",
      "                     nan,             nan,             nan],\n",
      "        [            nan, 1.17431740e-293, 7.51563135e-292, ...,\n",
      "                     nan, 1.53179810e-255,             nan],\n",
      "        ...,\n",
      "        [1.04045974e-311, 1.04046101e-311, 1.04046101e-311, ...,\n",
      "         1.04046101e-311, 1.04046096e-311, 1.04046094e-311],\n",
      "        [1.04046088e-311, 1.04046088e-311, 1.04046088e-311, ...,\n",
      "         1.04045961e-311, 1.04046085e-311, 1.04044295e-311],\n",
      "        [1.04045961e-311, 1.04044323e-311, 1.04045719e-311, ...,\n",
      "         1.04046114e-311, 1.04046124e-311, 1.04046124e-311]],\n",
      "\n",
      "       [[1.04046124e-311, 1.04046124e-311, 1.04046123e-311, ...,\n",
      "         1.04046094e-311, 1.04046114e-311, 1.04046114e-311],\n",
      "        [1.04046130e-311, 1.04046114e-311, 1.04046114e-311, ...,\n",
      "         1.04044154e-311, 1.04044170e-311, 1.04046130e-311],\n",
      "        [1.04046105e-311, 1.04044154e-311, 1.04044154e-311, ...,\n",
      "         1.04046149e-311, 1.04046144e-311, 1.04046140e-311],\n",
      "        ...,\n",
      "        [1.04043972e-311, 1.04048759e-311, 1.04043972e-311, ...,\n",
      "         1.04048768e-311, 1.04048769e-311, 1.04048769e-311],\n",
      "        [1.04048771e-311, 1.04048771e-311, 1.04048770e-311, ...,\n",
      "         1.04048768e-311, 1.04048768e-311, 1.04048768e-311],\n",
      "        [1.04048768e-311, 1.04048777e-311, 1.04048778e-311, ...,\n",
      "         1.04048778e-311, 1.04048777e-311, 1.04048778e-311]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "        [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
      "         0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
      "        ...,\n",
      "        [2.60588707e-284, 1.39899525e-282,             nan, ...,\n",
      "                     nan,             nan,             nan],\n",
      "        [            nan,             nan,             nan, ...,\n",
      "                     nan,             nan, 2.28116354e-305],\n",
      "        [1.75717096e-258,             nan,             nan, ...,\n",
      "                     nan, 3.85631123e-280,             nan]],\n",
      "\n",
      "       [[1.57128919e-250,             nan,             nan, ...,\n",
      "                     nan, 2.27599483e-298, 4.10635992e-300],\n",
      "        [            nan, 7.58413320e-295, 3.87340857e-304, ...,\n",
      "                     nan,             nan,             nan],\n",
      "        [3.28362944e-286,             nan,             nan, ...,\n",
      "         1.02218071e-282,             nan,             nan],\n",
      "        ...,\n",
      "        [9.87215674e-278, 3.63393878e-249,             nan, ...,\n",
      "                     nan, 5.14140154e-295, 3.03272446e-274],\n",
      "        [4.08302833e-302,             nan, 3.73305447e-301, ...,\n",
      "                     nan, 1.85159502e-298,             nan],\n",
      "        [            nan,             nan, 1.72443224e-307, ...,\n",
      "         8.02755475e-297, 2.15195838e-283,             nan]],\n",
      "\n",
      "       [[            nan, 2.36977072e-270, 3.30735319e-279, ...,\n",
      "                     nan, 1.91391953e-303,             nan],\n",
      "        [            nan,             nan,             nan, ...,\n",
      "                     nan, 2.19277589e-293,             nan],\n",
      "        [            nan, 7.76475331e-299,             nan, ...,\n",
      "                     nan,             nan,             nan],\n",
      "        ...,\n",
      "        [1.04047039e-311, 1.04047030e-311, 1.04047058e-311, ...,\n",
      "         1.04047070e-311, 1.04047069e-311, 1.04047073e-311],\n",
      "        [1.04047069e-311, 1.04047070e-311, 1.04047069e-311, ...,\n",
      "         1.04047065e-311, 1.04047058e-311, 1.04047073e-311],\n",
      "        [1.04047073e-311, 1.04046052e-311, 1.04047082e-311, ...,\n",
      "         1.04047082e-311, 1.04047073e-311, 1.04047073e-311]]])], array([7.93487397e-001, 2.78149850e-307, 6.23043089e-307, 1.33510967e-307,\n",
      "       1.27947604e-307, 2.22523004e-307, 4.45058486e-308, 8.45606157e-307,\n",
      "       1.60219035e-306, 1.60214553e-306, 2.22515025e-307, 1.37961302e-306,\n",
      "       2.22522868e-306, 9.34611148e-307, 3.11525958e-307, 1.69118108e-306,\n",
      "       8.06632139e-308, 1.20160711e-306, 1.69119330e-306, 1.29062500e-306,\n",
      "       8.90092016e-307, 1.60218763e-306, 6.23054972e-307, 2.22522597e-306,\n",
      "       1.06810268e-306, 6.23052935e-307, 1.33511018e-306, 1.60220393e-306,\n",
      "       8.45559303e-307, 1.06811422e-306, 1.05694828e-307, 1.42410974e-306]))\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_encoder.<locals>.Encoder.forward() missing 1 required positional argument: 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m ae_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32me:\\IITB\\SEM8\\ME793\\Drug Discovery\\Drug-Discovery\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\IITB\\SEM8\\ME793\\Drug Discovery\\Drug-Discovery\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[68], line 14\u001b[0m, in \u001b[0;36mMoleculeGenerator.forward\u001b[1;34m(self, mol_features)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mol_features):\n\u001b[1;32m---> 14\u001b[0m     z_mean, z_log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(z_mean, z_log_var)\n\u001b[0;32m     17\u001b[0m     reconstruction_adjacency, reconstruction_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n",
      "File \u001b[1;32me:\\IITB\\SEM8\\ME793\\Drug Discovery\\Drug-Discovery\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\IITB\\SEM8\\ME793\\Drug Discovery\\Drug-Discovery\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_encoder.<locals>.Encoder.forward() missing 1 required positional argument: 'features'"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import optim\n",
    "\n",
    "# Define the optimizer\n",
    "\n",
    "\n",
    "# Define the encoder and decoder\n",
    "encoder = get_encoder(\n",
    "    gconv_units=[9],\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    "    latent_dim=LATENT_DIM,\n",
    "    dense_units=[512],\n",
    "    dropout_rate=0.0,\n",
    "    features = \n",
    ")\n",
    "decoder = get_decoder(\n",
    "    dense_units=[128, 256, 512],\n",
    "    dropout_rate=0.2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM)\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\n",
    "ae_optimizer = optim.Adam(model.parameters(), lr=VAE_LR)\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()  # or whatever loss function is appropriate for your task\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        ae_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
